import requests
from bs4 import BeautifulSoup
import re
import random
import time
import logging
from datetime import datetime
import pytz
import jdatetime
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.common.exceptions import WebDriverException
import unicodedata
import socket
from concurrent.futures import ThreadPoolExecutor, as_completed
import json
import os

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

USER_AGENTS = [
    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/14.0 Safari/605.1.15',
    'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/92.0.4515.107 Safari/537.36'
]
HISTORY_LENGTH = 24  

def get_random_user_agent():
    return random.choice(USER_AGENTS)

def clean_line(line):
    line = line.strip().replace('\r', '').replace('\n', '')
    line = ''.join(c for c in line if unicodedata.category(c)[0] != 'C')
    return line

def check_proxy_status(server, port, timeout=3):
    """Check if a proxy server is online by attempting a connection."""
    start_time = time.time()
    try:
        sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
        sock.settimeout(timeout)
        result = sock.connect_ex((server, int(port)))
        sock.close()
        if result == 0:
            latency = round((time.time() - start_time) * 1000)  # Ø¨Ù‡ Ù…ÛŒÙ„ÛŒâ€ŒØ«Ø§Ù†ÛŒÙ‡
            logging.info(f"Proxy {server}:{port} is online ({latency}ms)")
            return {'status': 'Online', 'latency': latency}
        else:
            logging.warning(f"Proxy {server}:{port} is offline or unreachable")
            return {'status': 'Offline', 'latency': None}
    except (socket.timeout, socket.gaierror, ConnectionRefusedError, socket.error) as e:
        logging.error(f"Error checking proxy {server}:{port}: {e}")
        return {'status': 'Offline', 'latency': None}

def load_existing_proxies_history(filename='Files/extracted_proxies.json'):
    """Ø®ÙˆØ§Ù†Ø¯Ù† ØªØ§Ø±ÛŒØ®Ú†Ù‡ Ù¾Ø±ÙˆÚ©Ø³ÛŒâ€ŒÙ‡Ø§ Ø§Ø² ÙØ§ÛŒÙ„ JSON"""
    if os.path.exists(filename):
        try:
            with open(filename, 'r', encoding='utf-8') as file:
                data = json.load(file)
                return {proxy['tg_url']: proxy for proxy in data}
        except Exception as e:
            logging.error(f"Error reading {filename}: {e}")
    return {}

def fetch_proxies_from_text_urls(urls):
    all_proxies = []
    headers = {'User-Agent': get_random_user_agent()}
    pattern = r'^(tg://proxy|https://t\.me/proxy)\?server=[^&]+&port=\d+(&secret=.+)$'
    
    for url in urls:
        try:
            logging.info(f"Fetching proxies from {url}")
            response = requests.get(url, headers=headers, timeout=10)
            response.raise_for_status()
            lines = response.text.splitlines()
            proxy_checks = []
            
            for line in lines:
                line = clean_line(line)
                if not line:
                    continue
                if re.match(pattern, line):
                    match = re.match(r'^(?:tg://proxy|https://t\.me/proxy)\?server=([^&]+)&port=(\d+)&secret=([0-9a-fA-F]+.*)$', line)
                    if match:
                        server, port, secret = match.groups()
                        proxy_checks.append({
                            'tg_url': line,
                            'server': server,
                            'port': int(port),
                            'secret': secret
                        })
                    else:
                        logging.debug(f"Invalid or skipped proxy: {line} (does not match pattern)")
                else:
                    logging.debug(f"Invalid or skipped proxy: {line} (does not match pattern)")
            
            with ThreadPoolExecutor(max_workers=30) as executor:
                future_to_proxy = {executor.submit(check_proxy_status, proxy['server'], proxy['port']): proxy for proxy in proxy_checks}
                for future in as_completed(future_to_proxy):
                    proxy = future_to_proxy[future]
                    try:
                        status_result = future.result()
                        proxy['status'] = status_result['status']
                        proxy['latency'] = status_result['latency']
                        if status_result['status'] == 'Online':
                            all_proxies.append(proxy)
                    except Exception as e:
                        logging.error(f"Error checking proxy {proxy['tg_url']}: {e}")
                        proxy['status'] = 'Offline'
                        proxy['latency'] = None
            
            logging.info(f"Fetched {len(lines)} lines, {len(all_proxies)} valid and online MTProto proxies from {url}")
        except requests.RequestException as e:
            logging.error(f"HTTP error fetching {url}: {e}")
        time.sleep(random.uniform(0.5, 1.0))
    return all_proxies

def fetch_proxies_from_telegram_channel(url):
    proxies = []
    options = Options()
    options.add_argument('--headless')
    options.add_argument('--no-sandbox')
    options.add_argument('--disable-dev-shm-usage')
    options.add_argument(f'user-agent={get_random_user_agent()}')
    
    try:
        driver = webdriver.Chrome(options=options)
        driver.get(url)
        logging.info(f"Opened {url}")
        
        last_height = driver.execute_script("return document.body.scrollHeight")
        for i in range(5):
            driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
            time.sleep(2)
            new_height = driver.execute_script("return document.body.scrollHeight")
            logging.info(f"Scrolled {url}, attempt {i+1}, new height: {new_height}")
            if new_height == last_height:
                logging.info(f"No more content to load for {url}")
                break
            last_height = new_height
        
        page_source = driver.page_source
        if "CAPTCHA" in page_source or "recaptcha" in page_source.lower():
            logging.warning(f"CAPTCHA detected on {url}")
        
        soup = BeautifulSoup(page_source, 'html.parser')
        pattern = r'^(tg://proxy|https://t\.me/proxy)\?server=[^&]+&port=\d+(&secret=.+)$'
        proxy_elements = soup.find_all('a', href=re.compile(pattern))
        
        proxy_checks = []
        for element in proxy_elements:
            proxy_url = element.get('href')
            match = re.match(r'^(?:tg://proxy|https://t\.me/proxy)\?server=([^&]+)&port=(\d+)&secret=([0-9a-fA-F]+.*)$', proxy_url)
            if match:
                server, port, secret = match.groups()
                proxy_checks.append({
                    'tg_url': proxy_url,
                    'server': server,
                    'port': int(port),
                    'secret': secret
                })
        
        with ThreadPoolExecutor(max_workers=30) as executor:
            future_to_proxy = {executor.submit(check_proxy_status, proxy['server'], proxy['port']): proxy for proxy in proxy_checks}
            for future in as_completed(future_to_proxy):
                proxy = future_to_proxy[future]
                try:
                    status_result = future.result()
                    proxy['status'] = status_result['status']
                    proxy['latency'] = status_result['latency']
                    if status_result['status'] == 'Online':
                        proxies.append(proxy)
                except Exception as e:
                    logging.error(f"Error checking proxy {proxy['tg_url']}: {e}")
                    proxy['status'] = 'Offline'
                    proxy['latency'] = None
        
        logging.info(f"Fetched {len(proxies)} valid and online MTProto proxies from {url}")
    except WebDriverException as e:
        logging.error(f"WebDriver error fetching {url}: {e}")
    except Exception as e:
        logging.error(f"General error fetching {url}: {e}")
    finally:
        try:
            driver.quit()
        except:
            pass
    time.sleep(random.uniform(0.5, 1.0))
    return proxies

def save_proxies_to_file(proxy_list, filename='../proxy.txt'):
    try:
        existing_proxies_history = load_existing_proxies_history()
        
        for proxy in proxy_list:
            history = existing_proxies_history.get(proxy['tg_url'], {}).get('history', [])
            new_status_bit = 1 if proxy['status'] == 'Online' else 0
            history.append(new_status_bit)
            if len(history) > HISTORY_LENGTH:
                history = history[-HISTORY_LENGTH:]
            proxy['history'] = history
    
        with open('Files/extracted_proxies.json', 'w', encoding='utf-8') as file:
            json.dump(proxy_list, file, ensure_ascii=False, indent=4)
        logging.info(f"Saved {len(proxy_list)} proxies with history to Files/extracted_proxies.json")
        
        existing_proxies = []
        if os.path.exists(filename):
            with open(filename, 'r', encoding='utf-8') as file:
                existing_proxies = [line.strip() for line in file if line.strip()]
        
        unique_proxies = list(set(existing_proxies + [proxy['tg_url'] for proxy in proxy_list if proxy['status'] == 'Online']))

        with open(filename, 'w', encoding='utf-8') as file:
            for proxy in unique_proxies:
                file.write(proxy + '\n')
        logging.info(f"Saved {len(unique_proxies)} unique online proxies to {filename}")
        return proxy_list
    except IOError as e:
        logging.error(f"Error writing to {filename}: {e}")
        return []

def update_readme(proxy_list):
    try:
        utc_now = datetime.now(pytz.UTC)
        iran_tz = pytz.timezone('Asia/Tehran')
        iran_now = utc_now.astimezone(iran_tz)
        
        jalali_date = jdatetime.datetime.fromgregorian(datetime=iran_now)
        update_time_iran = jalali_date.strftime('%H:%M %d-%m-%Y')
        logging.info(f"Updating README with Iranian timestamp: {update_time_iran}")

        sample_proxies = random.sample(proxy_list, min(20, len(proxy_list))) if proxy_list else []
        table_rows = ""
        valid_proxies = 0
        for i, proxy in enumerate(sample_proxies, 1):
            proxy_url = proxy['tg_url'].replace('tg://proxy', 'https://t.me/proxy')
            status = proxy['status']
            latency = f"{proxy['latency']}ms" if proxy['latency'] else '-'
            match = re.match(r'^(?:tg://proxy|https://t\.me/proxy)\?server=([^&]+)&port=(\d+)&secret=([0-9a-fA-F]+.*)$', proxy['tg_url'])
            if match:
                server, port, secret = match.groups()
                display_proxy = f"https://t.me/proxy?server={server}&port={port}&secret={secret}"
                table_rows += f"| {i} | `{server}` | `{port}` | {status} ({latency}) | [Ù„ÛŒÙ†Ú© Ù¾Ø±ÙˆÚ©Ø³ÛŒ]({display_proxy}) |\n"
                valid_proxies += 1
                logging.info(f"Valid proxy added to table: {proxy_url} ({status}, {latency})")
            else:
                logging.warning(f"Invalid proxy format, skipped: {proxy['tg_url']}")
        
        logging.info(f"Added {valid_proxies} valid proxies to the table (out of {len(sample_proxies)} sampled)")

        readme_content = f"""# ğŸ“Š Ù†ØªØ§ÛŒØ¬ Ø§Ø³ØªØ®Ø±Ø§Ø¬: (Ø¢Ø®Ø±ÛŒÙ† Ø¨Ø±ÙˆØ²Ø±Ø³Ø§Ù†ÛŒ: {update_time_iran})

<p align="center">
  <img src="https://img.shields.io/badge/license-MIT-blue.svg" alt="MIT License" />
  <img src="https://img.shields.io/badge/python-3.9-blue" alt="Python 3.9" />
  <img src="https://img.shields.io/badge/contributions-welcome-brightgreen.svg?style=flat" alt="Contributions Welcome" />
  <img src="https://github.com/Poriya58p/telegram-proxy-scraper/actions/workflows/scraper.yml/badge.svg" alt="Proxy Scraper Workflow" />
  <img src="https://img.shields.io/github/last-commit/Argh94/telegram-proxy-scraper" alt="GitHub Last Commit" />
  <img src="https://img.shields.io/github/issues/Argh94/telegram-proxy-scraper" alt="GitHub Issues" />
</p>

Ø§ÛŒÙ† Ù¾Ø±ÙˆÚ˜Ù‡ ÛŒÚ© Ø§Ø³Ú©Ø±ÛŒÙ¾Øª Ù¾Ø§ÛŒØªÙˆÙ† Ø¨Ø±Ø§ÛŒ Ø¬Ù…Ø¹â€ŒØ¢ÙˆØ±ÛŒ Ø®ÙˆØ¯Ú©Ø§Ø± Ù¾Ø±ÙˆÚ©Ø³ÛŒâ€ŒÙ‡Ø§ÛŒ MTProto ØªÙ„Ú¯Ø±Ø§Ù… Ø§Ø² Ù…Ù†Ø§Ø¨Ø¹ Ù…ØªÙ†ÛŒ Ùˆ Ú©Ø§Ù†Ø§Ù„â€ŒÙ‡Ø§ÛŒ ØªÙ„Ú¯Ø±Ø§Ù… Ø§Ø³Øª. Ù¾Ø±ÙˆÚ©Ø³ÛŒâ€ŒÙ‡Ø§ Ø¯Ø± ÙØ§ÛŒÙ„ `proxy.txt` Ø°Ø®ÛŒØ±Ù‡ Ù…ÛŒâ€ŒØ´ÙˆÙ†Ø¯ Ùˆ Ù‡Ø± 3 Ø³Ø§Ø¹Øª Ø¨Ù‡â€ŒØµÙˆØ±Øª Ø®ÙˆØ¯Ú©Ø§Ø± Ø¨Ù‡â€ŒØ±ÙˆØ²Ø±Ø³Ø§Ù†ÛŒ Ù…ÛŒâ€ŒØ´ÙˆÙ†Ø¯.

## âœ¨ Ø¯Ø±Ø¨Ø§Ø±Ù‡ Ù¾Ø±ÙˆÚ˜Ù‡

Ø§ÛŒÙ† Ø§Ø³Ú©Ø±ÛŒÙ¾Øª Ø¨Ø§ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² `requests` Ø¨Ø±Ø§ÛŒ Ù…Ù†Ø§Ø¨Ø¹ Ù…ØªÙ†ÛŒ Ùˆ `selenium` Ø¨Ø±Ø§ÛŒ Ú©Ø§Ù†Ø§Ù„â€ŒÙ‡Ø§ÛŒ ØªÙ„Ú¯Ø±Ø§Ù…ØŒ Ù¾Ø±ÙˆÚ©Ø³ÛŒâ€ŒÙ‡Ø§ÛŒ MTProto Ø±Ø§ Ø¬Ù…Ø¹â€ŒØ¢ÙˆØ±ÛŒ Ù…ÛŒâ€ŒÚ©Ù†Ø¯. ÙÙ‚Ø· Ù¾Ø±ÙˆÚ©Ø³ÛŒâ€ŒÙ‡Ø§ÛŒ Ø¢Ù†Ù„Ø§ÛŒÙ† Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯Ù‡ Ùˆ Ù¾Ø±ÙˆÚ©Ø³ÛŒâ€ŒÙ‡Ø§ÛŒ ØªÚ©Ø±Ø§Ø±ÛŒ Ø­Ø°Ù Ù…ÛŒâ€ŒØ´ÙˆÙ†Ø¯. Ø§ÛŒÙ† ÙØ±Ø¢ÛŒÙ†Ø¯ Ø¨Ù‡â€ŒØµÙˆØ±Øª Ø®ÙˆØ¯Ú©Ø§Ø± Ø¨Ø§ **GitHub Actions** Ù‡Ø± 3 Ø³Ø§Ø¹Øª Ø§Ø¬Ø±Ø§ Ù…ÛŒâ€ŒØ´ÙˆØ¯.

## ğŸš€ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§
- ğŸŒ Ø¬Ù…Ø¹â€ŒØ¢ÙˆØ±ÛŒ Ù¾Ø±ÙˆÚ©Ø³ÛŒ Ø§Ø² Ù…Ù†Ø§Ø¨Ø¹ Ù…ØªÙ†ÛŒ Ùˆ Ú©Ø§Ù†Ø§Ù„â€ŒÙ‡Ø§ÛŒ ØªÙ„Ú¯Ø±Ø§Ù…
- âœ… Ø¨Ø±Ø±Ø³ÛŒ ÙˆØ¶Ø¹ÛŒØª Ù¾Ø±ÙˆÚ©Ø³ÛŒâ€ŒÙ‡Ø§ (Ø¢Ù†Ù„Ø§ÛŒÙ†/Ø¢ÙÙ„Ø§ÛŒÙ†)
- ğŸ”„ Ø¨Ù‡â€ŒØ±ÙˆØ²Ø±Ø³Ø§Ù†ÛŒ Ø®ÙˆØ¯Ú©Ø§Ø± Ù‡Ø± 3 Ø³Ø§Ø¹Øª
- ğŸ—‘ Ø­Ø°Ù Ù¾Ø±ÙˆÚ©Ø³ÛŒâ€ŒÙ‡Ø§ÛŒ ØªÚ©Ø±Ø§Ø±ÛŒ
- ğŸ”‘ Ø¨Ø¯ÙˆÙ† Ù†ÛŒØ§Ø² Ø¨Ù‡ API ØªÙ„Ú¯Ø±Ø§Ù…
- ğŸ“± Ù…Ù†Ø§Ø³Ø¨ Ø¨Ø±Ø§ÛŒ Ú©Ø§Ø±Ø¨Ø±Ø§Ù† Ø¯Ø± Ø¬Ø³ØªØ¬ÙˆÛŒ Ù¾Ø±ÙˆÚ©Ø³ÛŒâ€ŒÙ‡Ø§ÛŒ ÙØ¹Ø§Ù„ MTProto

## ğŸ“‹ Ù¾ÛŒØ´â€ŒÙ†ÛŒØ§Ø²Ù‡Ø§
- ğŸ Ù¾Ø§ÛŒØªÙˆÙ† 3.9
- ğŸ“¦ Ú©ØªØ§Ø¨Ø®Ø§Ù†Ù‡â€ŒÙ‡Ø§ÛŒ Ù…ÙˆØ±Ø¯ Ù†ÛŒØ§Ø²: `requests`, `beautifulsoup4`, `selenium`, `pytz`, `jdatetime`
- Ù†ØµØ¨ ÙˆØ§Ø¨Ø³ØªÚ¯ÛŒâ€ŒÙ‡Ø§ Ø¨Ø§: `pip install -r requirements.txt`

## ğŸ›  Ù†Ø­ÙˆÙ‡ Ø§Ø³ØªÙØ§Ø¯Ù‡
1. ÙØ§ÛŒÙ„ `proxy.txt` Ø±Ø§ Ø§Ø² [Ø§ÛŒÙ†Ø¬Ø§](proxy.txt) Ø¯Ø§Ù†Ù„ÙˆØ¯ Ú©Ù†ÛŒØ¯.
2. Ù„ÛŒÙ†Ú©â€ŒÙ‡Ø§ÛŒ Ù¾Ø±ÙˆÚ©Ø³ÛŒ (Ø¨Ø§ ÙØ±Ù…Øª `tg://proxy?...` ÛŒØ§ `https://t.me/proxy?...`) Ø±Ø§ Ø¯Ø± Ú©Ù„Ø§ÛŒÙ†Øª ØªÙ„Ú¯Ø±Ø§Ù… ÙˆØ§Ø±Ø¯ Ú©Ù†ÛŒØ¯.
3. Ø¯Ø± Ø¬Ø¯ÙˆÙ„ Ø²ÛŒØ±ØŒ Ø±ÙˆÛŒ Ù„ÛŒÙ†Ú©â€ŒÙ‡Ø§ÛŒ Ø³ØªÙˆÙ† **Ù„ÛŒÙ†Ú© Ù¾Ø±ÙˆÚ©Ø³ÛŒ** Ú©Ù„ÛŒÚ© Ú©Ù†ÛŒØ¯ ØªØ§ Ø¨Ù‡ ØªÙ„Ú¯Ø±Ø§Ù… Ù‡Ø¯Ø§ÛŒØª Ø´ÙˆÛŒØ¯ ÛŒØ§ Ù„ÛŒÙ†Ú© Ø±Ø§ Ú©Ù¾ÛŒ Ú©Ù†ÛŒØ¯.
4. Ø¨Ø±Ø§ÛŒ Ø¨Ù‡â€ŒØ±ÙˆØ²Ø±Ø³Ø§Ù†ÛŒ Ø¯Ø³ØªÛŒØŒ Ø¨Ù‡ ØªØ¨ **Actions** Ø¯Ø± Ù…Ø®Ø²Ù† Ø¨Ø±ÙˆÛŒØ¯ Ùˆ Ø±ÙˆÛŒ **Run workflow** Ú©Ù„ÛŒÚ© Ú©Ù†ÛŒØ¯.

## ğŸŒ Ù…Ù†Ø§Ø¨Ø¹ Ù¾Ø±ÙˆÚ©Ø³ÛŒ
- **Ù…Ù†Ø§Ø¨Ø¹ Ù…ØªÙ†ÛŒ**:
  - [MahsaNetConfigTopic](https://raw.githubusercontent.com/MahsaNetConfigTopic/proxy/main/proxies.txt)
  - [MhdiTaheri/ProxyCollector](https://raw.githubusercontent.com/MhdiTaheri/ProxyCollector/main/proxy.txt)
  - [SoliSpirit/mtproto](https://raw.githubusercontent.com/SoliSpirit/mtproto/master/all_proxies.txt)
- **Ú©Ø§Ù†Ø§Ù„â€ŒÙ‡Ø§ÛŒ ØªÙ„Ú¯Ø±Ø§Ù…**:
  - iporoto, HiProxy, iproxy, iRoProxy, proxyforopeta, IRN_Proxy, MProxy_ir, ProxyHagh, PyroProxy, ProxyMTProto, MTPro_XYZ, vpns, mtmvpn, asr_proxy, proxyskyy

## ğŸ“ˆ Ù†Ù…ÙˆÙ†Ù‡ Ù¾Ø±ÙˆÚ©Ø³ÛŒâ€ŒÙ‡Ø§
Ø¬Ø¯ÙˆÙ„ Ø²ÛŒØ± Ù†Ù…ÙˆÙ†Ù‡â€ŒØ§ÛŒ Ø§Ø² 20 Ù¾Ø±ÙˆÚ©Ø³ÛŒ ÙØ¹Ø§Ù„ Ø§Ø² ÙØ§ÛŒÙ„ `proxy.txt` Ø±Ø§ Ù†Ù…Ø§ÛŒØ´ Ù…ÛŒâ€ŒØ¯Ù‡Ø¯. Ø¨Ø±Ø§ÛŒ Ø§Ø³ØªÙØ§Ø¯Ù‡ØŒ Ø±ÙˆÛŒ Ù„ÛŒÙ†Ú© Ù¾Ø±ÙˆÚ©Ø³ÛŒ Ú©Ù„ÛŒÚ© Ú©Ù†ÛŒØ¯ ÛŒØ§ Ø¢Ù† Ø±Ø§ Ú©Ù¾ÛŒ Ú©Ù†ÛŒØ¯:

| # | Ø³Ø±ÙˆØ± (Server) | Ù¾ÙˆØ±Øª (Port) | ÙˆØ¶Ø¹ÛŒØª | Ù„ÛŒÙ†Ú© Ù¾Ø±ÙˆÚ©Ø³ÛŒ |
|---|---------------|-------------|-------|-------------|
{table_rows}

> **ğŸ’¡ Ù†Ú©ØªÙ‡**: Ø§ÛŒÙ† Ø¬Ø¯ÙˆÙ„ ÙÙ‚Ø· Ù†Ù…ÙˆÙ†Ù‡â€ŒØ§ÛŒ Ø§Ø² Ù¾Ø±ÙˆÚ©Ø³ÛŒâ€ŒÙ‡Ø§Ø³Øª. Ø¨Ø±Ø§ÛŒ Ø¯Ø³ØªØ±Ø³ÛŒ Ø¨Ù‡ Ù„ÛŒØ³Øª Ú©Ø§Ù…Ù„ Ùˆ Ø¨Ù‡â€ŒØ±ÙˆØ²ØŒ ÙØ§ÛŒÙ„ [proxy.txt](proxy.txt) Ø±Ø§ Ø¯Ø§Ù†Ù„ÙˆØ¯ Ú©Ù†ÛŒØ¯.

## ğŸ¤ Ù…Ø´Ø§Ø±Ú©Øª
Ø§Ø² Ø§ÛŒØ¯Ù‡â€ŒÙ‡Ø§ Ùˆ Ù…Ø´Ø§Ø±Ú©Øª Ø´Ù…Ø§ Ø§Ø³ØªÙ‚Ø¨Ø§Ù„ Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ…! Ø¨Ø±Ø§ÛŒ Ø¨Ù‡Ø¨ÙˆØ¯ Ù¾Ø±ÙˆÚ˜Ù‡:
1. ÛŒÚ© **Issue** Ø¯Ø± Ù…Ø®Ø²Ù† Ø¨Ø§Ø² Ú©Ù†ÛŒØ¯.
2. ÛŒØ§ ÛŒÚ© **Pull Request** Ø¨Ø§ ØªØºÛŒÛŒØ±Ø§Øª Ù¾ÛŒØ´Ù†Ù‡Ø§Ø¯ÛŒ Ø§Ø±Ø³Ø§Ù„ Ú©Ù†ÛŒØ¯.

## ğŸ“œ Ù„Ø§ÛŒØ³Ù†Ø³
Ø§ÛŒÙ† Ù¾Ø±ÙˆÚ˜Ù‡ ØªØ­Øª [Ù„Ø§ÛŒØ³Ù†Ø³ MIT](https://github.com/Argh94/telegram-proxy-scraper/blob/main/Files/LISENSE) Ù…Ù†ØªØ´Ø± Ø´Ø¯Ù‡ Ø§Ø³Øª.

## ğŸ”— Ù„ÛŒÙ†Ú©â€ŒÙ‡Ø§ÛŒ Ù…ÙÛŒØ¯
- ğŸ“„ [Ù„ÛŒØ³Øª Ù¾Ø±ÙˆÚ©Ø³ÛŒâ€ŒÙ‡Ø§](proxy.txt)
- ğŸš€ [ÙˆØ¶Ø¹ÛŒØª GitHub Actions](https://github.com/Argh94/telegram-proxy-scraper/actions)
- â­ [Ù…Ø§ Ø±Ø§ Ø³ØªØ§Ø±Ù‡ Ø¯Ù‡ÛŒØ¯!](https://github.com/Argh94/telegram-proxy-scraper)

## ğŸ“Š Stargazers Ø¯Ø± Ú¯Ø°Ø± Ø²Ù…Ø§Ù†
<p align="center">
  <img src="https://starchart.cc/Argh94/telegram-proxy-scraper.svg?variant=adaptive" alt="Stargazers over time" />
</p>

---

ğŸŒŸ **Ø³Ù¾Ø§Ø³ Ø§Ø² Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Telegram Proxy Scraper!** Ø§Ú¯Ø± Ø³Ø¤Ø§Ù„ÛŒ Ø¯Ø§Ø±ÛŒØ¯ØŒ Ø¯Ø± Ø¨Ø®Ø´ Issues Ù…Ø·Ø±Ø­ Ú©Ù†ÛŒØ¯.
"""

        with open('../README.md', 'w', encoding='utf-8') as file:
            file.write(readme_content)
        logging.info("Successfully updated README.md with new styling and Iranian date format")
    except Exception as e:
        logging.error(f"Error updating README.md: {e}")

if __name__ == "__main__":
    text_urls = [
        "https://raw.githubusercontent.com/MahsaNetConfigTopic/proxy/main/proxies.txt",
        "https://raw.githubusercontent.com/MhdiTaheri/ProxyCollector/main/proxy.txt",
        "https://raw.githubusercontent.com/SoliSpirit/mtproto/master/all_proxies.txt"
    ]
    
    telegram_urls = [
        "https://t.me/s/iporoto",
        "https://t.me/s/HiProxy",
        "https://t.me/s/iproxy",
        "https://t.me/s/iRoProxy",
        "https://t.me/s/proxyforopeta",
        "https://t.me/s/IRN_Proxy",
        "https://t.me/s/MProxy_ir",
        "https://t.me/s/ProxyHagh",
        "https://t.me/s/PyroProxy",
        "https://t.me/s/ProxyMTProto",
        "https://t.me/s/MTPro_XYZ",
        "https://t.me/s/vpns",
        "https://t.me/s/mtmvpn",
        "https://t.me/s/asr_proxy",
        "https://t.me/s/proxyskyy"
    ]
    
    text_proxies = fetch_proxies_from_text_urls(text_urls)
    
    telegram_proxies = []
    for url in telegram_urls:
        proxies = fetch_proxies_from_telegram_channel(url)
        telegram_proxies.extend(proxies)
    
    all_proxies = text_proxies + telegram_proxies
    
    all_proxies = save_proxies_to_file(all_proxies)
    
    update_readme(all_proxies)
