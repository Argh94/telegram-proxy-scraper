name: Telegram Proxy Scraper

on:
  schedule:
    - cron: '0 */3 * * *'
  push:
    branches: [ main ]
  workflow_dispatch:

permissions:
  contents: write

jobs:
  scrape:
    runs-on: ubuntu-latest
    timeout-minutes: 15

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.9'

      - name: Set up PHP
        uses: shivammathur/setup-php@v2
        with:
          php-version: '8.1'
          extensions: curl

      - name: Install Chrome
        uses: browser-actions/setup-chrome@v1
        with:
          chrome-version: stable
        id: setup-chrome

      - name: Verify ChromeDriver
        run: |
          chromedriver --version
          echo "Chrome installed at: ${{ steps.setup-chrome.outputs.chrome-path }}"
          echo "ChromeDriver installed at: ${{ steps.setup-chrome.outputs.chromedriver-path }}"

      - name: Ensure output files and directory exist
        run: |
          mkdir -p Files
          touch proxy.txt
          touch Files/extracted_proxies.json
          touch Files/offline_proxies.txt
          touch Files/usernames.json
          chmod -R 777 Files  # ÿßÿ∑ŸÖ€åŸÜÿßŸÜ ÿßÿ≤ ÿØÿ≥ÿ™ÿ±ÿ≥€å ŸÜŸàÿ¥ÿ™ŸÜ
          ls -l Files

      - name: Create or update usernames.json
        working-directory: ./Files
        run: |
          if [ ! -s usernames.json ]; then
            echo '[
                "NetAccount",
                "GlypeX",
                "ProxyMTProto",
                "iMTProto",
                "darkproxy",
                "TelMTProto",
                "MTP_roto",
                "ProxyMTProto_tel",
                "mtpproxyirani"
            ]' > usernames.json
            echo "Created usernames.json with default channels"
          else
            echo "usernames.json already exists"
          fi
          cat usernames.json

      - name: Create or update requirements.txt
        working-directory: ./Files
        run: |
          echo -e "requests\nbeautifulsoup4\nselenium\npytz\njdatetime" > requirements.txt
          cat requirements.txt

      - name: Install Python dependencies
        working-directory: ./Files
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt

      - name: Run PHP scraper
        working-directory: ./Files
        run: |
          php extract_proxies.php > php-scraper.log 2>&1
        continue-on-error: true

      - name: Upload PHP scraper log
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: php-scraper-log
          path: Files/php-scraper.log

      - name: Run Python scraper
        working-directory: ./Files
        run: |
          python main.py > python-scraper.log 2>&1
        continue-on-error: true

      - name: Upload Python scraper log
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: python-scraper-log
          path: Files/python-scraper.log

      - name: Commit and push changes
        run: |
          git config --global user.name 'github-actions[bot]'
          git config --global user.email '41898282+github-actions[bot]@users.noreply.github.com'
          git pull origin main --rebase || true
          git add proxy.txt README.md Files/requirements.txt Files/extracted_proxies.json Files/offline_proxies.txt Files/usernames.json || true
          git commit -m "‚õìÔ∏è‚Äçüí• Argh94 Update - PHP and Python proxies" || true
          git push || true
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        continue-on-error: true
