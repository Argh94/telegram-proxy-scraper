import requests
from bs4 import BeautifulSoup
import re
import random
import time
import logging
from datetime import datetime
import pytz
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.common.exceptions import WebDriverException
import unicodedata

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

USER_AGENTS = [
    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/14.0 Safari/605.1.15',
    'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/92.0.4515.107 Safari/537.36'
]

def get_random_user_agent():
    return random.choice(USER_AGENTS)

def clean_line(line):
    line = line.strip().replace('\r', '').replace('\n', '')
    line = ''.join(c for c in line if unicodedata.category(c)[0] != 'C')
    return line

def fetch_proxies_from_text_urls(urls):
    all_links = []
    headers = {'User-Agent': get_random_user_agent()}
    pattern = r'^(tg://proxy|https://t\.me/proxy)\?server=[^&]+&port=\d+(&secret=.+)$'
    
    for url in urls:
        try:
            logging.info(f"Fetching proxies from {url}")
            response = requests.get(url, headers=headers, timeout=10)
            response.raise_for_status()
            lines = response.text.splitlines()
            for line in lines:
                line = clean_line(line)
                if not line:
                    continue
                if re.match(pattern, line):
                    all_links.append(line)
                    logging.info(f"Valid proxy found: {line}")
                else:
                    logging.debug(f"Invalid or skipped proxy: {line} (does not match pattern)")
            logging.info(f"Fetched {len(lines)} lines, {len(all_links)} valid MTProto proxies from {url}")
        except requests.RequestException as e:
            logging.error(f"HTTP error fetching {url}: {e}")
        time.sleep(random.uniform(1, 3))
    return all_links

def fetch_proxies_from_telegram_channel(url):
    proxies = []
    options = Options()
    options.add_argument('--headless')
    options.add_argument('--no-sandbox')
    options.add_argument('--disable-dev-shm-usage')
    options.add_argument(f'user-agent={get_random_user_agent()}')
    
    try:
        driver = webdriver.Chrome(options=options)
        driver.get(url)
        logging.info(f"Opened {url}")
        
        last_height = driver.execute_script("return document.body.scrollHeight")
        for i in range(35):  
            driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
            time.sleep(7)  
            new_height = driver.execute_script("return document.body.scrollHeight")
            logging.info(f"Scrolled {url}, attempt {i+1}, new height: {new_height}")
            if new_height == last_height:
                logging.info(f"No more content to load for {url}")
                break
            last_height = new_height
        
        page_source = driver.page_source
        if "CAPTCHA" in page_source or "recaptcha" in page_source.lower():
            logging.warning(f"CAPTCHA detected on {url}")
        
        soup = BeautifulSoup(page_source, 'html.parser')
        pattern = r'^(tg://proxy|https://t\.me/proxy)\?server=[^&]+&port=\d+(&secret=.+)$'
        proxy_elements = soup.find_all('a', href=re.compile(pattern))
        proxies = [element.get('href') for element in proxy_elements]
        for proxy in proxies:
            logging.info(f"Valid proxy found from Telegram: {proxy}")
        logging.info(f"Fetched {len(proxies)} MTProto proxies from {url}")
    except WebDriverException as e:
        logging.error(f"WebDriver error fetching {url}: {e}")
    except Exception as e:
        logging.error(f"General error fetching {url}: {e}")
    finally:
        try:
            driver.quit()
        except:
            pass
    time.sleep(random.uniform(2, 5))
    return proxies

def save_proxies_to_file(proxy_list, filename='../proxy.txt'):
    try:
        unique_proxies = list(set(proxy_list))
        with open(filename, 'w', encoding='utf-8') as file:
            for proxy in unique_proxies:
                file.write(proxy + '\n')
        logging.info(f"Saved {len(unique_proxies)} unique proxies to {filename}")
        return unique_proxies
    except IOError as e:
        logging.error(f"Error writing to {filename}: {e}")
        return []

def update_readme(proxy_list):
    try:
        utc_now = datetime.now(pytz.UTC)
        iran_tz = pytz.timezone('Asia/Tehran')
        iran_now = utc_now.astimezone(iran_tz)
        update_time_utc = utc_now.strftime('%d %B %Y, %H:%M UTC')
        update_time_iran = iran_now.strftime('%H:%M')
        logging.info(f"Updating README with timestamp: {update_time_utc} (Iran: {update_time_iran})")

        sample_proxies = random.sample(proxy_list, min(20, len(proxy_list))) if proxy_list else []
        table_rows = ""
        for i, proxy in enumerate(sample_proxies, 1):
            match = re.match(r'^(tg://proxy|https://t\.me/proxy)\?server=([^&]+)&port=(\d+)&secret=.+$', proxy)
            if match:
                server, port = match.groups()[1:3]
                table_rows += f"| {i} | {server} | {port} | ÙØ¹Ø§Ù„ | `{proxy}` |\n"

        readme_content = f"""# Telegram Proxy Scraper

[![MIT License](https://img.shields.io/badge/license-MIT-blue.svg)](https://opensource.org/licenses/MIT)
[![Python](https://img.shields.io/badge/python-3.9-blue)](https://www.python.org/downloads/)
[![Contributions Welcome](https://img.shields.io/badge/contributions-welcome-brightgreen.svg?style=flat)](https://github.com/Argh94/telegram-proxy-scraper/issues)
[![Proxy Scraper Workflow](https://github.com/Poriya58p/telegram-proxy-scraper/actions/workflows/scraper.yml/badge.svg)](https://github.com/Argh94/telegram-proxy-scraper/actions/workflows/scraper.yml)
![GitHub last commit](https://img.shields.io/github/last-commit/Argh94/telegram-proxy-scraper)
![GitHub issues](https://img.shields.io/github/issues/Argh94/telegram-proxy-scraper)

**Ø¢Ø®Ø±ÛŒÙ† Ø¨Ù‡â€ŒØ±ÙˆØ²Ø±Ø³Ø§Ù†ÛŒ Ù¾Ø±ÙˆÚ©Ø³ÛŒâ€ŒÙ‡Ø§**: {update_time_utc} (Ø¨Ù‡ ÙˆÙ‚Øª Ø§ÛŒØ±Ø§Ù†: {update_time_iran})

Ø§ÛŒÙ† Ù¾Ø±ÙˆÚ˜Ù‡ ÛŒÙ‡ Ø§Ø³Ú©Ø±ÛŒÙ¾Øª Ù¾Ø§ÛŒØªÙˆÙ† Ø¨Ø±Ø§ÛŒ Ø¬Ù…Ø¹â€ŒØ¢ÙˆØ±ÛŒ Ø®ÙˆØ¯Ú©Ø§Ø± Ù¾Ø±ÙˆÚ©Ø³ÛŒâ€ŒÙ‡Ø§ÛŒ MTProto ØªÙ„Ú¯Ø±Ø§Ù… Ø§Ø² Ù…Ù†Ø§Ø¨Ø¹ Ù…ØªÙ†ÛŒ Ùˆ Ú©Ø§Ù†Ø§Ù„â€ŒÙ‡Ø§ÛŒ ØªÙ„Ú¯Ø±Ø§Ù…Ù‡. Ù¾Ø±ÙˆÚ©Ø³ÛŒâ€ŒÙ‡Ø§ ØªÙˆ ÙØ§ÛŒÙ„ `proxy.txt` Ø°Ø®ÛŒØ±Ù‡ Ù…ÛŒâ€ŒØ´Ù† Ùˆ Ù‡Ø± 3 Ø³Ø§Ø¹Øª Ø¨Ù‡â€ŒØµÙˆØ±Øª Ø®ÙˆØ¯Ú©Ø§Ø± Ø¨Ù‡â€ŒØ±ÙˆØ²Ø±Ø³Ø§Ù†ÛŒ Ù…ÛŒâ€ŒØ´Ù†.

## Ø¯Ø±Ø¨Ø§Ø±Ù‡ Ù¾Ø±ÙˆÚ˜Ù‡

Ø§ÛŒÙ† Ø§Ø³Ú©Ø±ÛŒÙ¾Øª Ø¨Ø§ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² `requests` Ø¨Ø±Ø§ÛŒ Ù…Ù†Ø§Ø¨Ø¹ Ù…ØªÙ†ÛŒ Ùˆ `selenium` Ø¨Ø±Ø§ÛŒ ØµÙØ­Ø§Øª ÙˆØ¨ Ú©Ø§Ù†Ø§Ù„â€ŒÙ‡Ø§ÛŒ ØªÙ„Ú¯Ø±Ø§Ù… (`t.me/s/...`) Ù¾Ø±ÙˆÚ©Ø³ÛŒâ€ŒÙ‡Ø§ÛŒ MTProto Ø±Ùˆ Ø¬Ù…Ø¹â€ŒØ¢ÙˆØ±ÛŒ Ù…ÛŒâ€ŒÚ©Ù†Ù‡. Ù¾Ø±ÙˆÚ©Ø³ÛŒâ€ŒÙ‡Ø§ÛŒ ØªÚ©Ø±Ø§Ø±ÛŒ Ø­Ø°Ù Ù…ÛŒâ€ŒØ´Ù† Ùˆ Ù†ØªÛŒØ¬Ù‡ ØªÙˆ ÙØ§ÛŒÙ„ `proxy.txt` Ø°Ø®ÛŒØ±Ù‡ Ù…ÛŒâ€ŒØ´Ù‡. ÙØ±Ø¢ÛŒÙ†Ø¯ Ø¨Ù‡â€ŒØµÙˆØ±Øª Ø®ÙˆØ¯Ú©Ø§Ø± Ø¨Ø§ **GitHub Actions** Ù‡Ø± 6 Ø³Ø§Ø¹Øª Ø§Ø¬Ø±Ø§ Ù…ÛŒâ€ŒØ´Ù‡.

## ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§
- Ø¬Ù…Ø¹â€ŒØ¢ÙˆØ±ÛŒ Ù¾Ø±ÙˆÚ©Ø³ÛŒ Ø§Ø² Ù…Ù†Ø§Ø¨Ø¹ Ù…ØªÙ†ÛŒ Ùˆ Ú©Ø§Ù†Ø§Ù„â€ŒÙ‡Ø§ÛŒ ØªÙ„Ú¯Ø±Ø§Ù…
- Ø¨Ù‡â€ŒØ±ÙˆØ²Ø±Ø³Ø§Ù†ÛŒ Ø®ÙˆØ¯Ú©Ø§Ø± Ù‡Ø± Û³ Ø³Ø§Ø¹Øª
- Ø­Ø°Ù Ù¾Ø±ÙˆÚ©Ø³ÛŒâ€ŒÙ‡Ø§ÛŒ ØªÚ©Ø±Ø§Ø±ÛŒ
- Ø¨Ø¯ÙˆÙ† Ù†ÛŒØ§Ø² Ø¨Ù‡ API ØªÙ„Ú¯Ø±Ø§Ù…
- Ù…Ù†Ø§Ø³Ø¨ Ø¨Ø±Ø§ÛŒ Ú©Ø§Ø±Ø¨Ø±Ø§Ù†ÛŒ Ú©Ù‡ Ø¨Ù‡ Ø¯Ù†Ø¨Ø§Ù„ Ù¾Ø±ÙˆÚ©Ø³ÛŒâ€ŒÙ‡Ø§ÛŒ MTProto ÙØ¹Ø§Ù„ Ù‡Ø³ØªÙ†

## Ù¾ÛŒØ´â€ŒÙ†ÛŒØ§Ø²Ù‡Ø§
- Ù¾Ø§ÛŒØªÙˆÙ† 3.9
- Ú©ØªØ§Ø¨Ø®ÙˆÙ†Ù‡â€ŒÙ‡Ø§ÛŒ Ù…ÙˆØ±Ø¯ Ù†ÛŒØ§Ø²: `requests`, `beautifulsoup4`, `selenium`, `pytz`
- ÙØ§ÛŒÙ„ `requirements.txt` Ø´Ø§Ù…Ù„ ØªÙ…Ø§Ù… ÙˆØ§Ø¨Ø³ØªÚ¯ÛŒâ€ŒÙ‡Ø§Ø³Øª.

## Ù†Ø­ÙˆÙ‡ Ø§Ø³ØªÙØ§Ø¯Ù‡
1. ÙØ§ÛŒÙ„ `proxy.txt` Ø±Ùˆ Ø§Ø² [Ø§ÛŒÙ†Ø¬Ø§](proxy.txt) Ø¯Ø§Ù†Ù„ÙˆØ¯ Ú©Ù†ÛŒØ¯.
2. Ù„ÛŒÙ†Ú©â€ŒÙ‡Ø§ÛŒ Ù¾Ø±ÙˆÚ©Ø³ÛŒ (Ø¨Ø§ ÙØ±Ù…Øª `tg://proxy?...` ÛŒØ§ `https://t.me/proxy?...`) Ø±Ùˆ ØªÙˆ Ú©Ù„Ø§ÛŒÙ†Øª ØªÙ„Ú¯Ø±Ø§Ù… Ø®ÙˆØ¯ØªÙˆÙ† ÙˆØ§Ø±Ø¯ Ú©Ù†ÛŒØ¯.
3. Ø¨Ø±Ø§ÛŒ Ú©Ù¾ÛŒ Ú©Ø±Ø¯Ù† Ù¾Ø±ÙˆÚ©Ø³ÛŒâ€ŒÙ‡Ø§ÛŒ Ø²ÛŒØ±ØŒ Ø±ÙˆÛŒ Ù„ÛŒÙ†Ú© ØªÙˆ Ø³ØªÙˆÙ† "Ù„ÛŒÙ†Ú© Ù¾Ø±ÙˆÚ©Ø³ÛŒ" Ù„Ù…Ø³ Ø·ÙˆÙ„Ø§Ù†ÛŒ Ú©Ù†ÛŒØ¯ Ùˆ Ú¯Ø²ÛŒÙ†Ù‡ "Copy" Ø±Ùˆ Ø§Ù†ØªØ®Ø§Ø¨ Ú©Ù†ÛŒØ¯ØŒ Ø³Ù¾Ø³ ØªÙˆ ØªÙ„Ú¯Ø±Ø§Ù… Ù¾ÛŒØ³Øª Ú©Ù†ÛŒØ¯.
4. Ø¨Ø±Ø§ÛŒ Ø¨Ù‡â€ŒØ±ÙˆØ²Ø±Ø³Ø§Ù†ÛŒ Ø¯Ø³ØªÛŒØŒ Ø¨Ù‡ ØªØ¨ **Actions** ØªÙˆ Ù…Ø®Ø²Ù† Ø¨Ø±ÛŒØ¯ Ùˆ Ø±ÙˆÛŒ **Run workflow** Ú©Ù„ÛŒÚ© Ú©Ù†ÛŒØ¯.

## Ù…Ù†Ø§Ø¨Ø¹ Ù¾Ø±ÙˆÚ©Ø³ÛŒ
- **Ù…Ù†Ø§Ø¨Ø¹ Ù…ØªÙ†ÛŒ**:
  - [MahsaNetConfigTopic](https://raw.githubusercontent.com/MahsaNetConfigTopic/proxy/main/proxies.txt)
  - [ALIILAPRO/MTProtoProxy](https://raw.githubusercontent.com/ALIILAPRO/MTProtoProxy/main/proxy-list.txt)
  - [MhdiTaheri/ProxyCollector](https://raw.githubusercontent.com/MhdiTaheri/ProxyCollector/main/proxy.txt)
  - [SoliSpirit/mtproto](https://raw.githubusercontent.com/SoliSpirit/mtproto/master/all_proxies.txt)
- **Ú©Ø§Ù†Ø§Ù„â€ŒÙ‡Ø§ÛŒ ØªÙ„Ú¯Ø±Ø§Ù…**:
  - iporoto, HiProxy, iproxy, iRoProxy, proxyforopeta, IRN_Proxy, MProxy_ir, ProxyHagh, PyroProxy, ProxyMTProto, MTPro_XYZ, vpns, mtmvpn

## Ù†Ù…ÙˆÙ†Ù‡ Ù¾Ø±ÙˆÚ©Ø³ÛŒâ€ŒÙ‡Ø§
Ø¬Ø¯ÙˆÙ„ Ø²ÛŒØ± 20 Ù¾Ø±ÙˆÚ©Ø³ÛŒ Ù†Ù…ÙˆÙ†Ù‡ Ø§Ø² ÙØ§ÛŒÙ„ `proxy.txt` Ø±Ùˆ Ù†Ø´ÙˆÙ† Ù…ÛŒâ€ŒØ¯Ù‡. Ø¨Ø±Ø§ÛŒ Ú©Ù¾ÛŒ Ú©Ø±Ø¯Ù† Ù„ÛŒÙ†Ú© Ù¾Ø±ÙˆÚ©Ø³ÛŒØŒ Ø±ÙˆÛŒ Ù…ØªÙ† ØªÙˆ Ø³ØªÙˆÙ† "Ù„ÛŒÙ†Ú© Ù¾Ø±ÙˆÚ©Ø³ÛŒ" Ù„Ù…Ø³ Ø·ÙˆÙ„Ø§Ù†ÛŒ Ú©Ù†ÛŒØ¯ Ùˆ "Copy" Ø±Ùˆ Ø§Ù†ØªØ®Ø§Ø¨ Ú©Ù†ÛŒØ¯:

| #  | Ø³Ø±ÙˆØ± (Server)       | Ù¾ÙˆØ±Øª (Port) | ÙˆØ¶Ø¹ÛŒØª     | Ù„ÛŒÙ†Ú© Ù¾Ø±ÙˆÚ©Ø³ÛŒ                     |
|----|---------------------|-------------|-----------|---------------------------------|
{table_rows}

> **ØªÙˆØ¬Ù‡**: Ø§ÛŒÙ† Ø¬Ø¯ÙˆÙ„ ÙÙ‚Ø· Ø¨Ø±Ø§ÛŒ Ù†Ù…Ø§ÛŒØ´ Ù†Ù…ÙˆÙ†Ù‡â€ŒØ³Øª. Ø¨Ø±Ø§ÛŒ Ø¯Ø³ØªØ±Ø³ÛŒ Ø¨Ù‡ Ù‡Ù…Ù‡ Ù¾Ø±ÙˆÚ©Ø³ÛŒâ€ŒÙ‡Ø§ÛŒ Ø¨Ù‡â€ŒØ±ÙˆØ²ØŒ ÙØ§ÛŒÙ„ [proxy.txt](proxy.txt) Ø±Ùˆ Ø¯Ø§Ù†Ù„ÙˆØ¯ Ú©Ù†ÛŒØ¯.

## Ù…Ø´Ø§Ø±Ú©Øª
Ø§Ø² Ù…Ø´Ø§Ø±Ú©Øª Ø´Ù…Ø§ Ø§Ø³ØªÙ‚Ø¨Ø§Ù„ Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ…! Ø§Ú¯Ù‡ Ø§ÛŒØ¯Ù‡â€ŒØ§ÛŒ Ø¨Ø±Ø§ÛŒ Ø¨Ù‡Ø¨ÙˆØ¯ Ø§Ø³Ú©Ø±ÛŒÙ¾Øª Ø¯Ø§Ø±ÛŒØ¯ ÛŒØ§ Ù…ÛŒâ€ŒØ®ÙˆØ§ÛŒØ¯ Ù…Ù†Ø§Ø¨Ø¹ Ø¬Ø¯ÛŒØ¯ÛŒ Ø§Ø¶Ø§ÙÙ‡ Ú©Ù†ÛŒØ¯:
1. ÛŒÙ‡ **Issue** ØªÙˆ Ù…Ø®Ø²Ù† Ø¨Ø§Ø² Ú©Ù†ÛŒØ¯.
2. ÛŒØ§ ÛŒÙ‡ **Pull Request** Ø¨Ø§ ØªØºÛŒÛŒØ±Ø§Øª Ù¾ÛŒØ´Ù†Ù‡Ø§Ø¯ÛŒ Ø¨ÙØ±Ø³ØªÛŒØ¯.

## Ù„Ø§ÛŒØ³Ù†Ø³
Ø§ÛŒÙ† Ù¾Ø±ÙˆÚ˜Ù‡ ØªØ­Øª [Ù„Ø§ÛŒØ³Ù†Ø³ MIT](LICENSE) Ù…Ù†ØªØ´Ø± Ø´Ø¯Ù‡.

## Ù„ÛŒÙ†Ú©â€ŒÙ‡Ø§ÛŒ Ù…ÙÛŒØ¯
- ğŸ“„ [Ù„ÛŒØ³Øª Ù¾Ø±ÙˆÚ©Ø³ÛŒâ€ŒÙ‡Ø§](proxy.txt)
- ğŸš€ [ÙˆØ¶Ø¹ÛŒØª GitHub Actions](https://github.com/Argh94/telegram-proxy-scraper/actions)
- â­ [Ù…Ø§ Ø±Ùˆ Ø³ØªØ§Ø±Ù‡ Ø¨Ø¯ÛŒØ¯!](https://github.com/Argh94/telegram-proxy-scraper)

## Stargazers Ø¯Ø± Ú¯Ø°Ø± Ø²Ù…Ø§Ù†
[![Stargazers over time](https://starchart.cc/Argh94/telegram-proxy-scraper.svg?variant=adaptive)](https://starchart.cc/Argh94/telegram-proxy-scraper)

---

Ø³Ù¾Ø§Ø³ Ø§Ø² Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² **Telegram Proxy Scraper**! Ø§Ú¯Ù‡ Ø³Ø¤Ø§Ù„ÛŒ Ø¯Ø§Ø±ÛŒØ¯ØŒ ØªÙˆ Ø¨Ø®Ø´ Issues Ù…Ø·Ø±Ø­ Ú©Ù†ÛŒØ¯. ğŸŒŸ
"""

        with open('../README.md', 'w', encoding='utf-8') as file:
            file.write(readme_content)
        logging.info("Successfully updated README.md in repository root with latest update time and proxy samples")
    except Exception as e:
        logging.error(f"Error updating README.md: {e}")

if __name__ == "__main__":
    text_urls = [
        "https://raw.githubusercontent.com/MahsaNetConfigTopic/proxy/main/proxies.txt",
        "https://raw.githubusercontent.com/ALIILAPRO/MTProtoProxy/main/proxy-list.txt",
        "https://raw.githubusercontent.com/MhdiTaheri/ProxyCollector/main/proxy.txt",
        "https://raw.githubusercontent.com/SoliSpirit/mtproto/master/all_proxies.txt"
    ]
    
    telegram_urls = [
        "https://t.me/s/iporoto",
        "https://t.me/s/HiProxy",
        "https://t.me/s/iproxy",
        "https://t.me/s/iRoProxy",
        "https://t.me/s/proxyforopeta",
        "https://t.me/s/IRN_Proxy",
        "https://t.me/s/MProxy_ir",
        "https://t.me/s/ProxyHagh",
        "https://t.me/s/PyroProxy",
        "https://t.me/s/ProxyMTProto",
        "https://t.me/s/MTPro_XYZ",
        "https://t.me/s/vpns",
        "https://t.me/s/mtmvpn",
        "https://t.me/s/asr_proxy",
        "https://t.me/s/proxyskyy"
    ]
    
    text_proxies = fetch_proxies_from_text_urls(text_urls)
    
    telegram_proxies = []
    for url in telegram_urls:
        proxies = fetch_proxies_from_telegram_channel(url)
        telegram_proxies.extend(proxies)
    
    all_proxies = list(set(text_proxies + telegram_proxies))
    
    all_proxies = save_proxies_to_file(all_proxies)
    
    update_readme(all_proxies)
